import pandas as pd
import sys
from sklearn.model_selection import train_test_split
from pipeline.preprocessing_pipeline import PreprocessingPipeline

def split_data_for_isolation_forest(X, y):
    """×—×œ×•×§×” ×¨× ×“×•××œ×™×ª ×¢×‘×•×¨ Isolation Forest"""
    print("Using random split for Isolation Forest...")
    
    # ×¨××©×™×ª: ×—×œ×§ ×œ-Train (60%) ×•-Temp (40%)
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.4, random_state=42, stratify=y
    )
    
    # ×©× ×™×ª: ×—×œ×§ ××ª Temp ×œ-Validation (20%) ×•-Test (20%)
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
    )
    
    return X_train, X_val, X_test, y_train, y_val, y_test

def split_data_for_lstm(X, y, employee_col='employee_id', date_col='date'):
    """×—×œ×•×§×” ×¢× ××™×•×Ÿ ×œ×¤×™ ×¢×•×‘×“ ×•×ª××¨×™×š ×¢×‘×•×¨ LSTM"""
    print("Using employee-ordered split for LSTM...")
    
    # ×¦×•×¨ DataFrame ×××•×—×“ ×œ××™×•×Ÿ
    combined_df = pd.concat([X, y], axis=1)
    
    # ××™×™×Ÿ ×œ×¤×™ employee_id ×•××– ×œ×¤×™ date
    if employee_col in combined_df.columns and date_col in combined_df.columns:
        combined_df = combined_df.sort_values(by=[employee_col, date_col])
        print(f"Data sorted by {employee_col} and {date_col}")
    elif employee_col in combined_df.columns:
        combined_df = combined_df.sort_values(by=employee_col)
        print(f"Data sorted by {employee_col} only (date column not found)")
    else:
        print(f"Warning: Employee column '{employee_col}' not found. Using original order.")
    
    # ×¤×¦×œ ×‘×—×–×¨×” ×œ-X ×•-y
    X_sorted = combined_df.drop(columns=[y.name])
    y_sorted = combined_df[y.name]
    
    n_samples = len(X_sorted)
    
    # ×—×œ×•×§×” ×–×× ×™×ª: 60% ×˜×¨×™×™×Ÿ, 20% ×•×œ×™×“×™×™×©×Ÿ, 20% ×˜×¡×˜
    train_end = int(0.6 * n_samples)
    val_end = int(0.8 * n_samples)
    
    X_train = X_sorted.iloc[:train_end]
    X_val = X_sorted.iloc[train_end:val_end]
    X_test = X_sorted.iloc[val_end:]
    
    y_train = y_sorted.iloc[:train_end]
    y_val = y_sorted.iloc[train_end:val_end]
    y_test = y_sorted.iloc[val_end:]
    
    return X_train, X_val, X_test, y_train, y_val, y_test

def main():
    # ×‘×“×•×§ ×× ×¦×•×™×Ÿ ×¤×¨××˜×¨ ×¢×‘×•×¨ ×¡×•×’ ×”××•×“×œ
    if len(sys.argv) < 2:
        print("Usage: python main.py <model_type>")
        print("Available model types: isolation-forest, lstm")
        sys.exit(1)
    
    model_type = sys.argv[1].lower()
    
    if model_type not in ['isolation-forest', 'lstm']:
        print("Error: Invalid model type. Use 'isolation-forest' or 'lstm'")
        sys.exit(1)
    
    print(f"Processing data for {model_type} model...")
    
    # ×˜×¢×Ÿ ××ª ×”×“××˜×”
    df = pd.read_csv('insider_threat_dataset.csv')
    print(f"Original dataset size: {len(df)} records")
        
    # ×”××¨×” ×œfloat32 ×›×“×™ ×œ×—×¡×•×š ×–×™×›×¨×•×Ÿ
    df = df.astype({col: 'float32' for col in df.select_dtypes(include=['float64']).columns})
        
    # ×”×¡×¨ ×¢××•×“×•×ª ×©×œ× ×¨×œ×•×•× ×˜×™×•×ª
    columns_to_drop = []
    if 'is_emp_malicious' in df.columns:
        columns_to_drop.append('is_emp_malicious')
        
    if 'modification_details' in df.columns:
        columns_to_drop.append('modification_details')

    if columns_to_drop:
        df = df.drop(columns=columns_to_drop)
        print(f"Dropped columns: {columns_to_drop}")    
    
    # ×”×›×Ÿ ××ª ×”×“××˜×” ×œ×¢×™×‘×•×“
    target_col = 'is_malicious'
    X = df.drop(columns=[target_col])
    y = df[target_col]
    
    print(f"\nPreparing X and y...")
    print(f"X shape: {X.shape}")
    print(f"y shape: {y.shape}")
        
    # ×¦×•×¨ ××ª ×”×¤×™×™×¤×œ×™×™×Ÿ ×¢× ×¡×•×’ ×”××•×“×œ
    print(f"\nCreating preprocessing pipeline for {model_type}...")
    pipeline = PreprocessingPipeline(model_type=model_type)
    
    # ×××Ÿ ××ª ×”×¤×™×™×¤×œ×™×™×Ÿ ×¢×œ ×›×œ ×”×“××˜×” (×¨×§ fit, ×œ× transform ×¢×“×™×™×Ÿ)
    print("\nğŸ”§ FITTING pipeline...")
    pipeline.fit(X, y)
    print("âœ… Pipeline fit completed!")
        
    # ×”×—×œ ××ª ×”×¤×™×™×¤×œ×™×™×Ÿ ×¢×œ ×›×œ ×”×“××˜×”
    print("\nğŸ”„ TRANSFORMING data...")
    X_processed = pipeline.transform(X)
    print("âœ… Pipeline transform completed!")
    
    print(f"\nChecking for missing values after preprocessing...")
    print(f"Records before cleaning: {len(X_processed)}")
    
    # ×•×™×“×•× ×©××™×Ÿ ×©×•×¨×•×ª ×‘×¢×œ×•×ª ×¢×¨×›×™× ×—×¡×¨×™×
    mask = ~X_processed.isnull().any(axis=1)
    X_clean = X_processed[mask]
    y_clean = y[mask]
    removed_count = len(X_processed) - len(X_clean)
    if removed_count > 0:
        print(f"\nâš ï¸ ATTENTION: {removed_count} records were removed due to missing values!")
                
    # ×¢×›×©×™×• ×—×œ×§ ××ª ×”×“××˜×” ×”× ×§×™×” ×œ×¤×™ ×¡×•×’ ×”××•×“×œ
    if model_type == 'isolation-forest':
        X_train, X_val, X_test, y_train, y_val, y_test = split_data_for_isolation_forest(X_clean, y_clean)
    elif model_type == 'lstm':
        X_train, X_val, X_test, y_train, y_val, y_test = split_data_for_lstm(X_clean, y_clean)
    
    print(f"\nDataset split sizes after preprocessing and cleaning:")
    print(f"Train: {len(X_train)} samples ({len(X_train)/len(X_clean)*100:.1f}%)")
    print(f"Validation: {len(X_val)} samples ({len(X_val)/len(X_clean)*100:.1f}%)")
    print(f"Test: {len(X_test)} samples ({len(X_test)/len(X_clean)*100:.1f}%)")
    
    # ×”×›×Ÿ DataFrames ×¢× ×”×˜×¨×’×˜
    train_df = pd.concat([
        X_train.reset_index(drop=True), 
        y_train.reset_index(drop=True)
    ], axis=1)

    val_df = pd.concat([
        X_val.reset_index(drop=True), 
        y_val.reset_index(drop=True)
    ], axis=1)

    test_df = pd.concat([
        X_test.reset_index(drop=True), 
        y_test.reset_index(drop=True)
    ], axis=1)
    
    # ×©××•×¨ ×§×‘×¦×™ CSV
    train_filename = f'train_processed_{model_type.replace("-", "_")}.csv'
    val_filename = f'val_processed_{model_type.replace("-", "_")}.csv'
    test_filename = f'test_processed_{model_type.replace("-", "_")}.csv'
    
    train_df.to_csv(train_filename, index=False)
    val_df.to_csv(val_filename, index=False)
    test_df.to_csv(test_filename, index=False)
    
    print(f"\nFinal processed dataset shapes:")
    print(f"Train shape: {train_df.shape}")
    print(f"Validation shape: {val_df.shape}")
    print(f"Test shape: {test_df.shape}")
    
    # ×‘×“×•×§ ××ª ×”×ª×¤×œ×’×•×ª ×”×˜×¨×’×˜ ×‘×›×œ ×¡×˜
    print(f"\nTarget distribution:")
    print(f"Train - Positive: {y_train.sum()}/{len(y_train)} ({y_train.mean()*100:.2f}%)")
    print(f"Val - Positive: {y_val.sum()}/{len(y_val)} ({y_val.mean()*100:.2f}%)")
    print(f"Test - Positive: {y_test.sum()}/{len(y_test)} ({y_test.mean()*100:.2f}%)")
    
    print(f"\nFiles saved:")
    print(f"- {train_filename}") 
    print(f"- {val_filename}")
    print(f"- {test_filename}")

if __name__ == '__main__':
    main()